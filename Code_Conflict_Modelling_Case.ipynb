{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Appendix F}\n",
    "\\begin{lstlisting}[language=R]\n",
    "\n",
    "MIDB <- read.csv(\"MIDB 5.0.csv\")\n",
    "\\\\attach(MIDB)\n",
    " \\\\View(MIDB)\n",
    " \\\\MIDB[,c(\"ccode\", \"stday\", \"stmon\", \"endday\", \"endmon\", \"sidea\", \"revstate\", \"revtype1\", \"revtype2\", \"fatality\", \"fatalpre\", \"hiact\", \"hostlev\", \"orig\", \"version\")] <- list(NULL)\n",
    " \\\\View(MIDB)\n",
    " \\\\library(tidyverse)\n",
    " \\\\MIDB2<- MIDB %>% \n",
    "  \\\\ mutate(year = map2(styear, endyear, `:`)) %>% \n",
    "   \\\\select(-styear, -endyear) %>% \n",
    "   \\\\unnest\n",
    "\\\\# Other option\n",
    "\\\\# MIDB2 =  MIDB %>%\n",
    "\\\\#  +     rowwise() %>%\n",
    "\\\\#  +     mutate(year = list(seq(styear, endyear, 1))) %>%\n",
    "\\\\#  +     ungroup() %>%\n",
    "\\\\#  +     select(-styear, -endyear) %>%\n",
    "\\\\#  +     unnest()\n",
    " \\\\attach(MIDB2)\n",
    " \\\\MIDBTable <- table(stabb,year)\n",
    " \\\\MIDBTable2 <-xtabs(~stabb+year)\n",
    " \\\\write.csv(MIDBTable2, \"MIDBTABLE.csv\")\n",
    " \\\\MIDBFREQ<-read.csv(\"MIDBTABLE.csv\")\n",
    " \\\\View(MIDBFREQ)\n",
    " \\\\#add missing years (where nothing happens) in excel by hand\n",
    " \n",
    " \\\\MIDBTable3<-as.data.frame(MIDBTable)\n",
    "\\\\ MIDBTable3$Freq<-ifelse(MIDBTable3$Freq>0,1,0)\n",
    "\\\\ MIDBDUMMYTABLE<-xtabs(MIDBTable3$Freq~MIDBTable3$stabb+MIDBTable3$year)\n",
    "\\\\ write.csv(MIDBDUMMYTABLE,\"DUMMYTABLE.csv\")\n",
    " \\\\#again add missing years by hand in excel\n",
    "\\end{lstlisting}\n",
    "\n",
    "\\section*{Appendix G}\n",
    "\n",
    "\\begin{lstlisting}[language=Python]\n",
    "\n",
    "###### ARIMA MODELS #######################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score as r2_score\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.ar_model import ar_select_order\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "df_MIDB = pd.read_csv('BINARYTABLE.csv', sep=\";\")\n",
    "\n",
    "# df_MIDB\n",
    "\n",
    "country = 'USA'\n",
    "\n",
    "## ACF & PACF\n",
    "\n",
    "plot_acf(df_MIDB[country], lags=20)\n",
    "plt.show()\n",
    "\n",
    "plot_pacf(df_MIDB[country], lags=20)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Stationarity Test ADF ##################################\n",
    "\n",
    "class StationarityTests:\n",
    "    def __init__(self, significance=.05):\n",
    "        self.SignificanceLevel = significance\n",
    "        self.pValue = None\n",
    "        self.isStationary = None\n",
    "        \n",
    "    def ADF_Stationarity_Test(self, timeseries, printResults = True):\n",
    "        #Dickey-Fuller test:\n",
    "        adfTest = adfuller(timeseries, autolag='AIC')\n",
    "        \n",
    "        self.pValue = adfTest[1]\n",
    "        \n",
    "        if (self.pValue<self.SignificanceLevel):\n",
    "            self.isStationary = True\n",
    "        else:\n",
    "            self.isStationary = False\n",
    "        \n",
    "        if printResults:\n",
    "            dfResults = pd.Series(adfTest[0:4], index=['ADF Test Statistic','P-Value','# Lags Used','# Observations Used'])\n",
    "            #Add Critical Values\n",
    "            for key,value in adfTest[4].items():\n",
    "                dfResults['Critical Value (%s)'%key] = value\n",
    "            print('Augmented Dickey-Fuller Test Results:')\n",
    "            print(dfResults)\n",
    "\n",
    "sTest = StationarityTests(significance=0.05)\n",
    "sTest.ADF_Stationarity_Test(df_MIDB[country], printResults = True)\n",
    "print(\"Is the {} time series stationary? {}\".format(country, sTest.isStationary))\n",
    "print()\n",
    "\n",
    "\n",
    "## AR ###################################\n",
    "\n",
    "lags = [4]\n",
    "res = AutoReg(df_MIDB[country], lags=lags, old_names=False).fit()\n",
    "print(res.summary())\n",
    "\n",
    "\n",
    "forecast = res.predict(start= len(df_MIDB), end=len(df_MIDB) + 5)\n",
    "model_estimate = res.predict(start= 0, end=len(df_MIDB))\n",
    "\n",
    "plt.plot(model_estimate)\n",
    "plt.plot(df_MIDB[country])\n",
    "plt.plot(forecast)\n",
    "plt.legend([\"Model estimate\", \"True Data\", \"Forecast\"])\n",
    "plt.show()\n",
    "print(forecast)\n",
    "\n",
    "\n",
    "## ARIMA with lags from plots ACF, PACF #####################\n",
    "\n",
    "country_arima = df_MIDB[country]\n",
    "\n",
    "model_arima = ARIMA(country_arima, order = (4,0,0)).fit()\n",
    "print(model_arima.summary())\n",
    "\n",
    "\n",
    "forecast_arima = model_arima.predict(start= len(df_MIDB), end=len(df_MIDB) + 5)\n",
    "arima_estimate = model_arima.predict(start= 1, end=len(df_MIDB))\n",
    "\n",
    "\n",
    "plt.plot(arima_estimate)\n",
    "plt.plot(country_arima)\n",
    "plt.plot(forecast_arima)\n",
    "plt.legend([\"Model estimate\", \"True Data\", \"Forecast\"])\n",
    "plt.show()\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(country_arima,arima_estimate)\n",
    "print('r2: %f' % r2)\n",
    "forecast_arima\n",
    "\n",
    "\n",
    "### ARIMA with arbitrary initial lags = 5 #####################\n",
    "\n",
    "country_arima = df_MIDB[country]\n",
    "\n",
    "model_arima = ARIMA(country_arima, order = ((1,1,0,1,0),0,(1,1,0,1,0))).fit()\n",
    "print(model_arima.summary())\n",
    "\n",
    "\n",
    "forecast_arima = model_arima.predict(start= len(df_MIDB), end=len(df_MIDB) + 5)\n",
    "arima_estimate = model_arima.predict(start= 1, end=len(df_MIDB))\n",
    "\n",
    "\n",
    "plt.plot(arima_estimate)\n",
    "plt.plot(country_arima)\n",
    "plt.plot(forecast_arima)\n",
    "plt.legend([\"Model estimate\", \"True Data\", \"Forecast\"])\n",
    "plt.show()\n",
    "plt.savefig(country + 'arma_pred.png')\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(country_arima,arima_estimate)\n",
    "print('r2: %f' % r2)\n",
    "forecast_arima\n",
    "\n",
    "\n",
    "### LOGIT #######################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score as r2_score\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import patsy as patsy\n",
    "from patsy import ModelDesc\n",
    "from patsy import dmatrices\n",
    "from patsy import ModelDesc, Term, EvalFactor\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from statsmodels.discrete.discrete_model import Probit\n",
    "import operator\n",
    "import math\n",
    "import seaborn as sns\n",
    "import statsmodels\n",
    "\n",
    "\n",
    "## Data #########################\n",
    "\n",
    "df_MIDBFR = pd.read_csv('frequencyMID.csv',sep=\";\")\n",
    "df_MIDB = pd.read_csv('BINARYTABLE.csv', sep=\";\")\n",
    "Year = df_MIDBFR['YEAR']\n",
    "\n",
    "## Lagged Variables #############\n",
    "\n",
    "country = 'USA'\n",
    "FirstPredictedYear = 2015\n",
    "row= FirstPredictedYear - 1816\n",
    "row2= row + 1\n",
    "row3= row2 + 1\n",
    "row4= row3 + 1\n",
    "row5= row4 + 1\n",
    "inter_confl = df_MIDB[country]\n",
    "s3 = pd.Series([np.nan,np.nan,np.nan,np.nan,np.nan])\n",
    "inter_confl=inter_confl.append(s3,ignore_index=True)\n",
    "inter_confl=inter_confl.rename(country + \"Conflict\")\n",
    "\n",
    "inter_freq = df_MIDBFR[country]\n",
    "s3 = pd.Series([np.nan,np.nan,np.nan,np.nan,np.nan])\n",
    "inter_freq=inter_freq.append(s3,ignore_index=True)\n",
    "inter_freq=inter_freq.rename(country + \"Frequency\")\n",
    "inter_freq1 = inter_freq.shift(1)\n",
    "inter_freq1 = inter_freq1.rename(\"L1_\"+ country +\"Frequency\")\n",
    "inter_freq2 = inter_freq1.shift(1)\n",
    "inter_freq2 = inter_freq2.rename(\"L2_\"+ country +\"Frequency\")\n",
    "inter_freq3 = inter_freq2.shift(1)\n",
    "inter_freq3 = inter_freq3.rename(\"L3_\"+ country +\"Frequency\")\n",
    "inter_freq4 = inter_freq3.shift(1)\n",
    "inter_freq4 = inter_freq4.rename(\"L4_\"+ country +\"Frequency\")\n",
    "inter_freq5 = inter_freq4.shift(1)\n",
    "inter_freq5 = inter_freq5.rename(\"L5_\"+ country +\"Frequency\")\n",
    "\n",
    "inter_lagged = inter_confl.shift(1)\n",
    "inter_lagged = inter_lagged.rename(\"L1_\" + country + \"Conflict\")\n",
    "inter_lagged2 = inter_lagged.shift(1)\n",
    "inter_lagged2 = inter_lagged2.rename(\"L2_\"+ country +\"Conflict\")\n",
    "inter_lagged3 = inter_lagged2.shift(1)\n",
    "inter_lagged3 = inter_lagged3.rename(\"L3_\"+ country +\"Conflict\")\n",
    "inter_lagged4 = inter_lagged3.shift(1)\n",
    "inter_lagged4 = inter_lagged4.rename(\"L4_\"+ country +\"Conflict\")\n",
    "inter_lagged5 = inter_lagged4.shift(1)\n",
    "inter_lagged5 = inter_lagged5.rename(\"L5_\"+ country +\"Conflict\")\n",
    "data_confl = pd.concat([inter_confl, inter_lagged, inter_lagged2, inter_lagged3, inter_lagged4, inter_lagged5, inter_freq, inter_freq1, inter_freq2, inter_freq3, inter_freq4, inter_freq5],axis=1)\n",
    "\n",
    "\n",
    "## Poisson Regression for Frequency ####################\n",
    "\n",
    "## Poisson to predict frequency of unobserved year #####\n",
    "\n",
    "Z, K = dmatrices(country + 'Frequency'+ '~' + 'L1_'+country+'Frequency + L2_' +country+'Frequency + L3_' +country+ 'Frequency + L4_' \n",
    "                 +country+ 'Frequency + L5_' +country+ 'Frequency', NA_action=patsy.NAAction(NA_types=[]), data=data_confl, return_type='dataframe')\n",
    "\n",
    "def remove_most_insignificant(df, results):\n",
    "    max_p_value = max(results.pvalues.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    df.drop(columns = max_p_value, inplace = True)\n",
    "    return df\n",
    "\n",
    "insignificant_feature = True\n",
    "while insignificant_feature:\n",
    "    modelFR = sm.Poisson(Z, K,missing='drop')\n",
    "    resultsFR = modelFR.fit(cov_type='HC3')\n",
    "    significant = [p_value < 0.05 for p_value in resultsFR.pvalues[1:]]\n",
    "    if all(significant):\n",
    "        insignificant_feature = False\n",
    "    else:\n",
    "        if K.shape[1] == 1:\n",
    "            print('No significant features found')\n",
    "            resultsFR = None\n",
    "            insignificant_feature = False\n",
    "        else:\n",
    "            K = remove_most_insignificant(K, resultsFR)\n",
    "\n",
    "print(resultsFR.summary())\n",
    "\n",
    "signif_valuesFR = resultsFR.params.to_frame()\n",
    "signif_valuesFR = signif_valuesFR.reset_index()\n",
    "signif_valuesFR.columns = ['sign_variable','coef']\n",
    "\n",
    "\n",
    "zeta_1 = signif_valuesFR[signif_valuesFR['sign_variable'] == 'L1_'+ country + 'Frequency'] ['coef'].values\n",
    "zeta_2 = signif_valuesFR[signif_valuesFR['sign_variable'] == 'L2_'+ country + 'Frequency'] ['coef'].values\n",
    "zeta_3 = signif_valuesFR[signif_valuesFR['sign_variable'] == 'L3_'+ country + 'Frequency'] ['coef'].values\n",
    "zeta_4 = signif_valuesFR[signif_valuesFR['sign_variable'] == 'L4_'+ country + 'Frequency'] ['coef'].values\n",
    "zeta_5 = signif_valuesFR[signif_valuesFR['sign_variable'] == 'L5_'+ country + 'Frequency'] ['coef'].values\n",
    "\n",
    "interceptFR = signif_valuesFR[signif_valuesFR['sign_variable'] == 'Intercept'] ['coef'].values\n",
    "if zeta_1.size <= 0:\n",
    "    zeta_1 = 0\n",
    "\n",
    "if zeta_2.size <= 0:\n",
    "    zeta_2 = 0\n",
    "\n",
    "if zeta_3.size <= 0:\n",
    "    zeta_3 = 0\n",
    "\n",
    "if zeta_4.size <= 0:\n",
    "    zeta_4 = 0\n",
    "\n",
    "if zeta_5.size <= 0:\n",
    "    zeta_5 = 0\n",
    "\n",
    "Frequency1year = interceptFR + (zeta_1 * data_confl['L1_' +country+ 'Frequency'][row]) + (zeta_2 * data_confl['L2_' +country+ 'Frequency'][row]) + (zeta_3 * data_confl['L3_' +country+ 'Frequency'][row]) + (zeta_4 * data_confl['L4_' +country+ 'Frequency'][row]) + (zeta_5 * data_confl['L5_' +country+ 'Frequency'][row])\n",
    "Frequency1year = math.exp(Frequency1year)\n",
    "Frequency2year = interceptFR + (zeta_1 * Frequency1year) + (zeta_2 * data_confl['L2_' +country+ 'Frequency'][row2]) + (zeta_3 * data_confl['L3_' +country+ 'Frequency'][row2]) + (zeta_4 * data_confl['L4_' +country+ 'Frequency'][row2]) + (zeta_5 * data_confl['L5_' +country+ 'Frequency'][row2])\n",
    "Frequency2year = math.exp(Frequency2year)\n",
    "Frequency3year = interceptFR + (zeta_1 * Frequency2year) + (zeta_2 * Frequency1year) + (zeta_3 * data_confl['L3_' +country+ 'Frequency'][row3]) + (zeta_4 * data_confl['L4_' +country+ 'Frequency'][row3]) + (zeta_5 * data_confl['L5_' +country+ 'Frequency'][row3])\n",
    "Frequency3year = math.exp(Frequency3year)\n",
    "Frequency4year = interceptFR + (zeta_1 * Frequency3year) + (zeta_2 * Frequency2year) + (zeta_3 * Frequency1year) + (zeta_4 * data_confl['L4_' +country+ 'Frequency'][row4]) + (zeta_5 * data_confl['L5_' +country+ 'Frequency'][row4])\n",
    "Frequency4year = math.exp(Frequency4year)\n",
    "Frequency5year = interceptFR + (zeta_1 * Frequency4year) + (zeta_2 * Frequency3year) + (zeta_3 * Frequency2year) + (zeta_4 * Frequency1year) + (zeta_5 * data_confl['L5_' +country+ 'Frequency'][row5])\n",
    "Frequency5year = math.exp(Frequency5year)\n",
    "print(Frequency1year)\n",
    "print(Frequency2year)\n",
    "print(Frequency3year)\n",
    "print(Frequency4year)\n",
    "print(Frequency5year)\n",
    "\n",
    "\n",
    "## Logit ######################\n",
    "\n",
    "#Now that we have all data we can use LOGIT to predict\n",
    "Y, X = dmatrices(country + 'Conflict'+ '~' + ' L1_'+ country + 'Conflict + L2_'+ country + 'Conflict + L3_'+ country +'Conflict + L4_'+ country +\n",
    "                 'Conflict + L5_'+ country +'Conflict + L1_'+country+'Frequency + L2_' +country+'Frequency + L3_' +country+ 'Frequency + L4_' \n",
    "                 +country+ 'Frequency + L5_' +country+ 'Frequency', NA_action=patsy.NAAction(NA_types=[]), data=data_confl, return_type='dataframe')\n",
    "\n",
    "\n",
    "\n",
    "def remove_most_insignificant(df, results):\n",
    "    max_p_value = max(results.pvalues.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    df.drop(columns = max_p_value, inplace = True)\n",
    "    return df\n",
    "\n",
    "insignificant_feature = True\n",
    "while insignificant_feature:\n",
    "    model = sm.Logit(Y, X,missing='drop')\n",
    "    results = model.fit()\n",
    "    significant = [p_value < 0.05 for p_value in results.pvalues[1:]]\n",
    "    if all(significant):\n",
    "        insignificant_feature = False\n",
    "    else:\n",
    "        if X.shape[1] == 1:\n",
    "            print('No significant features found')\n",
    "            results = None\n",
    "            insignificant_feature = False\n",
    "        else:\n",
    "            X = remove_most_insignificant(X, results)\n",
    "\n",
    "print(results.summary())\n",
    "\n",
    "signif_values = results.params.to_frame()\n",
    "signif_values = signif_values.reset_index()\n",
    "signif_values.columns = ['sign_variable','coef']\n",
    "beta1 = signif_values[signif_values['sign_variable'] == 'L1_' + country + 'Conflict'] ['coef'].values\n",
    "beta2 = signif_values[signif_values['sign_variable'] == 'L2_' + country + 'Conflict'] ['coef'].values\n",
    "beta3 = signif_values[signif_values['sign_variable'] == 'L3_' + country + 'Conflict'] ['coef'].values\n",
    "beta4 = signif_values[signif_values['sign_variable'] == 'L4_' + country + 'Conflict'] ['coef'].values\n",
    "beta5 = signif_values[signif_values['sign_variable'] == 'L5_' + country + 'Conflict'] ['coef'].values\n",
    "\n",
    "theta1 = signif_values[signif_values['sign_variable'] == 'L1_'+ country + 'Frequency'] ['coef'].values\n",
    "theta2 = signif_values[signif_values['sign_variable'] == 'L2_'+ country + 'Frequency'] ['coef'].values\n",
    "theta3 = signif_values[signif_values['sign_variable'] == 'L3_'+ country + 'Frequency'] ['coef'].values\n",
    "theta4 = signif_values[signif_values['sign_variable'] == 'L4_'+ country + 'Frequency'] ['coef'].values\n",
    "theta5 = signif_values[signif_values['sign_variable'] == 'L5_'+ country + 'Frequency'] ['coef'].values\n",
    "\n",
    "intercept = signif_values[signif_values['sign_variable'] == 'Intercept'] ['coef'].values\n",
    "\n",
    "if theta1.size <= 0:\n",
    "    theta1 = 0\n",
    "    \n",
    "if theta2.size <= 0:\n",
    "    theta2 = 0\n",
    "    \n",
    "if theta3.size <= 0:\n",
    "    theta3 = 0\n",
    "    \n",
    "if theta4.size <= 0:\n",
    "    theta4 = 0\n",
    "    \n",
    "if theta5.size <= 0:\n",
    "    theta5 = 0\n",
    "\n",
    "if beta1.size <= 0:\n",
    "    beta1 = 0\n",
    "    \n",
    "if beta2.size <= 0:\n",
    "    beta2 = 0\n",
    "    \n",
    "if beta3.size <= 0:\n",
    "    beta3 = 0\n",
    "    \n",
    "if beta4.size <= 0:\n",
    "    beta4 = 0\n",
    "    \n",
    "if beta5.size <= 0:\n",
    "    beta5 = 0\n",
    "\n",
    "Y1year = intercept + beta1 * data_confl['L1_' +country+ 'Conflict'][row] + beta2 * data_confl['L2_' +country+ 'Conflict'][row] + beta3 * data_confl['L3_' +country+ 'Conflict'][row] + beta4 * data_confl['L4_' +country+ 'Conflict'][row] + beta5 * data_confl['L5_' +country+ 'Conflict'][row] + theta1 * data_confl['L1_' +country+ 'Frequency'][row] + theta2 * data_confl['L2_' +country+ 'Frequency'][row] + theta3 * data_confl['L3_' +country+ 'Frequency'][row] + theta4 * data_confl['L4_' +country+ 'Frequency'][row] + theta5 * data_confl['L5_' +country+ 'Frequency'][row]\n",
    "P1year = math.exp(Y1year)/(1+(math.exp(Y1year)))\n",
    "Y2year = intercept + beta1 * P1year + beta2 * data_confl['L2_' +country+ 'Conflict'][row2] + beta3 * data_confl['L3_' +country+ 'Conflict'][row2] + beta4 * data_confl['L4_' +country+ 'Conflict'][row2] + beta5 * data_confl['L5_' +country+ 'Conflict'][row2] + theta1 * Frequency1year + theta2 * data_confl['L2_' +country+ 'Frequency'][row2] + theta3 * data_confl['L3_' +country+ 'Frequency'][row2] + theta4 * data_confl['L4_' +country+ 'Frequency'][row2] + theta5 * data_confl['L5_' +country+ 'Frequency'][row2]\n",
    "P2year = math.exp(Y2year)/(1+(math.exp(Y2year)))\n",
    "Y3year = intercept + beta1 * P2year + beta2 * P1year + beta3 * data_confl['L3_' +country+ 'Conflict'][row3] + beta4 * data_confl['L4_' +country+ 'Conflict'][row3] + beta5 * data_confl['L5_' +country+ 'Conflict'][row3] + theta1 * Frequency2year + theta2 * Frequency1year + theta3 * data_confl['L3_' +country+ 'Frequency'][row3] + theta4 * data_confl['L4_' +country+ 'Frequency'][row3] + theta5 * data_confl['L5_' +country+ 'Frequency'][row3]\n",
    "P3year = math.exp(Y3year)/(1+(math.exp(Y3year)))\n",
    "Y4year = intercept + beta1 * P3year + beta2 * P2year + beta3 * P1year + beta4 * data_confl['L4_' +country+ 'Conflict'][row4] + beta5 * data_confl['L5_' +country+ 'Conflict'][row4] + theta1 * Frequency3year + theta2 * Frequency2year + theta3 * Frequency1year + theta4 * data_confl['L4_' +country+ 'Frequency'][row4] + theta5 * data_confl['L5_' +country+ 'Frequency'][row4]\n",
    "P4year = math.exp(Y4year)/(1+(math.exp(Y4year)))\n",
    "Y5year = intercept + beta1 * P4year + beta2 * P3year + beta3 * P2year + beta4 * P1year + beta5 * data_confl['L5_' +country+ 'Conflict'][row5] + theta1 * Frequency4year + theta2 * Frequency3year + theta3 * Frequency2year + theta4 * Frequency1year + theta5 * data_confl['L5_' +country+ 'Frequency'][row5]\n",
    "P5year = math.exp(Y5year)/(1+(math.exp(Y5year)))\n",
    "\n",
    "# probabilities #######\n",
    "\n",
    "print(P1year)\n",
    "print(P2year)\n",
    "print(P3year)\n",
    "print(P4year)\n",
    "print(P5year)\n",
    "\n",
    "## AIC & BIC #####\n",
    "\n",
    "print(results.aic)\n",
    "print(results.bic)\n",
    "\n",
    "\n",
    "pred=results.predict()\n",
    "preds=pd.DataFrame(pred)\n",
    "forecast1=[P1year,P2year,P3year,P4year,P5year]\n",
    "preds = preds.append(forecast1,ignore_index=True)\n",
    "startValue=204-len(preds)\n",
    "\n",
    "\n",
    "realData=df_MIDB[country]\n",
    "indexYear = pd.read_csv('YearIndex.csv')[startValue:]\n",
    "index2 = df_MIDB['YEAR'][:]\n",
    "wide_df2 = pd.DataFrame(realData)\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.scatterplot(y=realData,x=index2)\n",
    "sns.regplot(x=indexYear,y=preds,logistic=True,scatter=True,color='red')\n",
    "plt.title('Regression Line of Conflict for ' + country + ' Logit')\n",
    "plt.xlabel('Year') \n",
    "plt.ylabel('probabilities')\n",
    "plt.savefig(country + '_logit_predict.png')\n",
    "\n",
    "\n",
    "preds2 = [P1year,P2year,P3year,P4year,P5year]\n",
    "x = ['2015','2016','2017','2018','2019']\n",
    "plt.title('Forecasted probabilities for 5 years ' + country + ' Logit')\n",
    "plt.xlabel('Year') \n",
    "plt.ylabel('Probabilities')\n",
    "plt.grid()\n",
    "plt.plot(x,preds2, 'o-', markeredgewidth=0)\n",
    "plt.savefig(country + '_5years_predict_logit.png')\n",
    "\n",
    "\n",
    "## PROBIT #################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score as r2_score\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import patsy as patsy\n",
    "from patsy import ModelDesc\n",
    "from patsy import dmatrices\n",
    "from patsy import ModelDesc, Term, EvalFactor\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from statsmodels.discrete.discrete_model import Probit\n",
    "import operator\n",
    "import math\n",
    "import seaborn as sns\n",
    "import statsmodels\n",
    "\n",
    "## Data ###############\n",
    "\n",
    "df_MIDBFR = pd.read_csv('frequencyMID.csv',sep=\";\")\n",
    "df_MIDB = pd.read_csv('BINARYTABLE.csv', sep=\";\")\n",
    "Year = df_MIDBFR['YEAR']\n",
    "\n",
    "## Lagged Variables ###########\n",
    "\n",
    "country = 'USA'\n",
    "FirstPredictedYear = 2015\n",
    "row= FirstPredictedYear - 1816\n",
    "row2= row + 1\n",
    "row3= row2 + 1\n",
    "row4= row3 + 1\n",
    "row5= row4 + 1\n",
    "inter_confl = df_MIDB[country]\n",
    "s3 = pd.Series([np.nan,np.nan,np.nan,np.nan,np.nan])\n",
    "inter_confl=inter_confl.append(s3,ignore_index=True)\n",
    "inter_confl=inter_confl.rename(country + \"Conflict\")\n",
    "\n",
    "inter_freq = df_MIDBFR[country]\n",
    "s3 = pd.Series([np.nan,np.nan,np.nan,np.nan,np.nan])\n",
    "inter_freq=inter_freq.append(s3,ignore_index=True)\n",
    "inter_freq=inter_freq.rename(country + \"Frequency\")\n",
    "inter_freq1 = inter_freq.shift(1)\n",
    "inter_freq1 = inter_freq1.rename(\"L1_\"+ country +\"Frequency\")\n",
    "inter_freq2 = inter_freq1.shift(1)\n",
    "inter_freq2 = inter_freq2.rename(\"L2_\"+ country +\"Frequency\")\n",
    "inter_freq3 = inter_freq2.shift(1)\n",
    "inter_freq3 = inter_freq3.rename(\"L3_\"+ country +\"Frequency\")\n",
    "inter_freq4 = inter_freq3.shift(1)\n",
    "inter_freq4 = inter_freq4.rename(\"L4_\"+ country +\"Frequency\")\n",
    "inter_freq5 = inter_freq4.shift(1)\n",
    "inter_freq5 = inter_freq5.rename(\"L5_\"+ country +\"Frequency\")\n",
    "\n",
    "inter_lagged = inter_confl.shift(1)\n",
    "inter_lagged = inter_lagged.rename(\"L1_\" + country + \"Conflict\")\n",
    "inter_lagged2 = inter_lagged.shift(1)\n",
    "inter_lagged2 = inter_lagged2.rename(\"L2_\"+ country +\"Conflict\")\n",
    "inter_lagged3 = inter_lagged2.shift(1)\n",
    "inter_lagged3 = inter_lagged3.rename(\"L3_\"+ country +\"Conflict\")\n",
    "inter_lagged4 = inter_lagged3.shift(1)\n",
    "inter_lagged4 = inter_lagged4.rename(\"L4_\"+ country +\"Conflict\")\n",
    "inter_lagged5 = inter_lagged4.shift(1)\n",
    "inter_lagged5 = inter_lagged5.rename(\"L5_\"+ country +\"Conflict\")\n",
    "data_confl = pd.concat([inter_confl, inter_lagged, inter_lagged2, inter_lagged3, inter_lagged4, inter_lagged5, inter_freq, inter_freq1, inter_freq2, inter_freq3, inter_freq4, inter_freq5],axis=1)\n",
    "\n",
    "\n",
    "### Poisson Regression for Frequency #################\n",
    "\n",
    "# Poisson to predict frequency of unobserved year ######\n",
    "\n",
    "Z, K = dmatrices(country + 'Frequency'+ '~' + 'L1_'+country+'Frequency + L2_' +country+'Frequency + L3_' +country+ 'Frequency + L4_' \n",
    "                 +country+ 'Frequency + L5_' +country+ 'Frequency', NA_action=patsy.NAAction(NA_types=[]), data=data_confl, return_type='dataframe')\n",
    "\n",
    "def remove_most_insignificant(df, results):\n",
    "    max_p_value = max(results.pvalues.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    df.drop(columns = max_p_value, inplace = True)\n",
    "    return df\n",
    "\n",
    "insignificant_feature = True\n",
    "while insignificant_feature:\n",
    "    modelFR = sm.Poisson(Z, K,missing='drop')\n",
    "    resultsFR = modelFR.fit(cov_type='HC3')\n",
    "    significant = [p_value < 0.05 for p_value in resultsFR.pvalues[1:]]\n",
    "    if all(significant):\n",
    "        insignificant_feature = False\n",
    "    else:\n",
    "        if K.shape[1] == 1:\n",
    "            print('No significant features found')\n",
    "            resultsFR = None\n",
    "            insignificant_feature = False\n",
    "        else:\n",
    "            K = remove_most_insignificant(K, resultsFR)\n",
    "\n",
    "print(resultsFR.summary())\n",
    "\n",
    "signif_valuesFR = resultsFR.params.to_frame()\n",
    "signif_valuesFR = signif_valuesFR.reset_index()\n",
    "signif_valuesFR.columns = ['sign_variable','coef']\n",
    "\n",
    "\n",
    "zeta_1 = signif_valuesFR[signif_valuesFR['sign_variable'] == 'L1_'+ country + 'Frequency'] ['coef'].values\n",
    "zeta_2 = signif_valuesFR[signif_valuesFR['sign_variable'] == 'L2_'+ country + 'Frequency'] ['coef'].values\n",
    "zeta_3 = signif_valuesFR[signif_valuesFR['sign_variable'] == 'L3_'+ country + 'Frequency'] ['coef'].values\n",
    "zeta_4 = signif_valuesFR[signif_valuesFR['sign_variable'] == 'L4_'+ country + 'Frequency'] ['coef'].values\n",
    "zeta_5 = signif_valuesFR[signif_valuesFR['sign_variable'] == 'L5_'+ country + 'Frequency'] ['coef'].values\n",
    "\n",
    "interceptFR = signif_valuesFR[signif_valuesFR['sign_variable'] == 'Intercept'] ['coef'].values\n",
    "if zeta_1.size <= 0:\n",
    "    zeta_1 = 0\n",
    "\n",
    "if zeta_2.size <= 0:\n",
    "    zeta_2 = 0\n",
    "\n",
    "if zeta_3.size <= 0:\n",
    "    zeta_3 = 0\n",
    "\n",
    "if zeta_4.size <= 0:\n",
    "    zeta_4 = 0\n",
    "\n",
    "if zeta_5.size <= 0:\n",
    "    zeta_5 = 0\n",
    "\n",
    "Frequency1year = interceptFR + (zeta_1 * data_confl['L1_' +country+ 'Frequency'][row]) + (zeta_2 * data_confl['L2_' +country+ 'Frequency'][row]) + (zeta_3 * data_confl['L3_' +country+ 'Frequency'][row]) + (zeta_4 * data_confl['L4_' +country+ 'Frequency'][row]) + (zeta_5 * data_confl['L5_' +country+ 'Frequency'][row])\n",
    "Frequency1year = math.exp(Frequency1year)\n",
    "Frequency2year = interceptFR + (zeta_1 * Frequency1year) + (zeta_2 * data_confl['L2_' +country+ 'Frequency'][row2]) + (zeta_3 * data_confl['L3_' +country+ 'Frequency'][row2]) + (zeta_4 * data_confl['L4_' +country+ 'Frequency'][row2]) + (zeta_5 * data_confl['L5_' +country+ 'Frequency'][row2])\n",
    "Frequency2year = math.exp(Frequency2year)\n",
    "Frequency3year = interceptFR + (zeta_1 * Frequency2year) + (zeta_2 * Frequency1year) + (zeta_3 * data_confl['L3_' +country+ 'Frequency'][row3]) + (zeta_4 * data_confl['L4_' +country+ 'Frequency'][row3]) + (zeta_5 * data_confl['L5_' +country+ 'Frequency'][row3])\n",
    "Frequency3year = math.exp(Frequency3year)\n",
    "Frequency4year = interceptFR + (zeta_1 * Frequency3year) + (zeta_2 * Frequency2year) + (zeta_3 * Frequency1year) + (zeta_4 * data_confl['L4_' +country+ 'Frequency'][row4]) + (zeta_5 * data_confl['L5_' +country+ 'Frequency'][row4])\n",
    "Frequency4year = math.exp(Frequency4year)\n",
    "Frequency5year = interceptFR + (zeta_1 * Frequency4year) + (zeta_2 * Frequency3year) + (zeta_3 * Frequency2year) + (zeta_4 * Frequency1year) + (zeta_5 * data_confl['L5_' +country+ 'Frequency'][row5])\n",
    "Frequency5year = math.exp(Frequency5year)\n",
    "print(Frequency1year)\n",
    "print(Frequency2year)\n",
    "print(Frequency3year)\n",
    "print(Frequency4year)\n",
    "print(Frequency5year)\n",
    "\n",
    "## Probit ##########################\n",
    "\n",
    "#Now that we have all data we can use PROBIT to predict##\n",
    "Y, X = dmatrices(country + 'Conflict'+ '~' + ' L1_'+ country + 'Conflict + L2_'+ country + 'Conflict + L3_'+ country +'Conflict + L4_'+ country +\n",
    "                 'Conflict + L5_'+ country +'Conflict + L1_'+country+'Frequency + L2_' +country+'Frequency + L3_' +country+ 'Frequency + L4_' \n",
    "                 +country+ 'Frequency + L5_' +country+ 'Frequency', NA_action=patsy.NAAction(NA_types=[]), data=data_confl, return_type='dataframe')\n",
    "\n",
    "\n",
    "def remove_most_insignificant(df, results):\n",
    "    max_p_value = max(results.pvalues.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    df.drop(columns = max_p_value, inplace = True)\n",
    "    return df\n",
    "\n",
    "insignificant_feature = True\n",
    "while insignificant_feature:\n",
    "    model = sm.Probit(Y, X,missing='drop')\n",
    "    results = model.fit()\n",
    "    significant = [p_value < 0.05 for p_value in results.pvalues[1:]]\n",
    "    if all(significant):\n",
    "        insignificant_feature = False\n",
    "    else:\n",
    "        if X.shape[1] == 1:\n",
    "            print('No significant features found')\n",
    "            results = None\n",
    "            insignificant_feature = False\n",
    "        else:\n",
    "            X = remove_most_insignificant(X, results)\n",
    "\n",
    "print(results.summary())\n",
    "\n",
    "signif_values = results.params.to_frame()\n",
    "signif_values = signif_values.reset_index()\n",
    "signif_values.columns = ['sign_variable','coef']\n",
    "beta1 = signif_values[signif_values['sign_variable'] == 'L1_' + country + 'Conflict'] ['coef'].values\n",
    "beta2 = signif_values[signif_values['sign_variable'] == 'L2_' + country + 'Conflict'] ['coef'].values\n",
    "beta3 = signif_values[signif_values['sign_variable'] == 'L3_' + country + 'Conflict'] ['coef'].values\n",
    "beta4 = signif_values[signif_values['sign_variable'] == 'L4_' + country + 'Conflict'] ['coef'].values\n",
    "beta5 = signif_values[signif_values['sign_variable'] == 'L5_' + country + 'Conflict'] ['coef'].values\n",
    "\n",
    "theta1 = signif_values[signif_values['sign_variable'] == 'L1_'+ country + 'Frequency'] ['coef'].values\n",
    "theta2 = signif_values[signif_values['sign_variable'] == 'L2_'+ country + 'Frequency'] ['coef'].values\n",
    "theta3 = signif_values[signif_values['sign_variable'] == 'L3_'+ country + 'Frequency'] ['coef'].values\n",
    "theta4 = signif_values[signif_values['sign_variable'] == 'L4_'+ country + 'Frequency'] ['coef'].values\n",
    "theta5 = signif_values[signif_values['sign_variable'] == 'L5_'+ country + 'Frequency'] ['coef'].values\n",
    "\n",
    "intercept = signif_values[signif_values['sign_variable'] == 'Intercept'] ['coef'].values\n",
    "\n",
    "if theta1.size <= 0:\n",
    "    theta1 = 0\n",
    "    \n",
    "if theta2.size <= 0:\n",
    "    theta2 = 0\n",
    "    \n",
    "if theta3.size <= 0:\n",
    "    theta3 = 0\n",
    "    \n",
    "if theta4.size <= 0:\n",
    "    theta4 = 0\n",
    "    \n",
    "if theta5.size <= 0:\n",
    "    theta5 = 0\n",
    "\n",
    "if beta1.size <= 0:\n",
    "    beta1 = 0\n",
    "    \n",
    "if beta2.size <= 0:\n",
    "    beta2 = 0\n",
    "    \n",
    "if beta3.size <= 0:\n",
    "    beta3 = 0\n",
    "    \n",
    "if beta4.size <= 0:\n",
    "    beta4 = 0\n",
    "    \n",
    "if beta5.size <= 0:\n",
    "    beta5 = 0\n",
    "\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "Y1year= intercept + beta1 * data_confl['L1_' +country+ 'Conflict'][row] + beta2 * data_confl['L2_' +country+ 'Conflict'][row] + beta3 * data_confl['L3_' +country+ 'Conflict'][row] + beta4 * data_confl['L4_' +country+ 'Conflict'][row] + beta5 * data_confl['L5_' +country+ 'Conflict'][row] + theta1 * data_confl['L1_' +country+ 'Frequency'][row] + theta2 * data_confl['L2_' +country+ 'Frequency'][row] + theta3 * data_confl['L3_' +country+ 'Frequency'][row] + theta4 * data_confl['L4_' +country+ 'Frequency'][row] + theta5 * data_confl['L5_' +country+ 'Frequency'][row]\n",
    "P1year = st.norm.cdf(Y1year)\n",
    "Y2year = intercept + beta1 * P1year + beta2 * data_confl['L2_' +country+ 'Conflict'][row2] + beta3 * data_confl['L3_' +country+ 'Conflict'][row2] + beta4 * data_confl['L4_' +country+ 'Conflict'][row2] + beta5 * data_confl['L5_' +country+ 'Conflict'][row2] + theta1 * Frequency1year + theta2 * data_confl['L2_' +country+ 'Frequency'][row2] + theta3 * data_confl['L3_' +country+ 'Frequency'][row2] + theta4 * data_confl['L4_' +country+ 'Frequency'][row2] + theta5 * data_confl['L5_' +country+ 'Frequency'][row2]\n",
    "P2year = st.norm.cdf(Y2year)\n",
    "Y3year = intercept + beta1 * P2year + beta2 * P1year + beta3 * data_confl['L3_' +country+ 'Conflict'][row3] + beta4 * data_confl['L4_' +country+ 'Conflict'][row3] + beta5 * data_confl['L5_' +country+ 'Conflict'][row3] + theta1 * Frequency2year + theta2 * Frequency1year + theta3 * data_confl['L3_' +country+ 'Frequency'][row3] + theta4 * data_confl['L4_' +country+ 'Frequency'][row3] + theta5 * data_confl['L5_' +country+ 'Frequency'][row3]\n",
    "P3year = st.norm.cdf(Y3year)\n",
    "Y4year = intercept + beta1 * P3year + beta2 * P2year + beta3 * P1year + beta4 * data_confl['L4_' +country+ 'Conflict'][row4] + beta5 * data_confl['L5_' +country+ 'Conflict'][row4] + theta1 * Frequency3year + theta2 * Frequency2year + theta3 * Frequency1year + theta4 * data_confl['L4_' +country+ 'Frequency'][row4] + theta5 * data_confl['L5_' +country+ 'Frequency'][row4]\n",
    "P4year = st.norm.cdf(Y4year)\n",
    "Y5year = intercept + beta1 * P4year + beta2 * P3year + beta3 * P2year + beta4 * P1year + beta5 * data_confl['L5_' +country+ 'Conflict'][row5] + theta1 * Frequency4year + theta2 * Frequency3year + theta3 * Frequency2year + theta4 * Frequency1year + theta5 * data_confl['L5_' +country+ 'Frequency'][row5]\n",
    "P5year = st.norm.cdf(Y5year)\n",
    "print(P1year)\n",
    "print(P2year)\n",
    "print(P3year)\n",
    "print(P4year)\n",
    "print(P5year)\n",
    "\n",
    "## AIC & BIC #############\n",
    "\n",
    "print(results.aic)\n",
    "print(results.bic)\n",
    "\n",
    "pred=results.predict()\n",
    "preds=pd.DataFrame(pred)\n",
    "forecast1=[P1year,P2year,P3year,P4year,P5year]\n",
    "preds = preds.append(forecast1,ignore_index=True)\n",
    "startValue=204-len(preds)\n",
    "\n",
    "realData=df_MIDB[country]\n",
    "indexYear = pd.read_csv('YearIndex.csv')[startValue:]\n",
    "index2 = df_MIDB['YEAR'][:]\n",
    "wide_df2 = pd.DataFrame(realData)\n",
    "plt.figure(figsize=(18,8))\n",
    "sns.scatterplot(y=realData,x=index2)\n",
    "sns.regplot(x=indexYear,y=preds,logistic=True,scatter=True,color='red')\n",
    "plt.title('Regression Line of Conflict for ' + country + ' Probit')\n",
    "plt.xlabel('Year') \n",
    "plt.ylabel('Probabilities')\n",
    "plt.savefig(country + '_probit_predict.png')\n",
    "\n",
    "preds2 = [P1year,P2year,P3year,P4year,P5year]\n",
    "x = ['2015','2016','2017','2018','2019']\n",
    "plt.title('Forecasted probabilities for 5 years ' + country + ' Probit')\n",
    "plt.xlabel('Year') \n",
    "plt.ylabel('Probabilities')\n",
    "plt.grid()\n",
    "plt.plot(x,preds2, 'o-', markeredgewidth=0)\n",
    "plt.savefig(country + '_5years_predict_probit.png')\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "diff = len(data_confl)-5 -len(pred)\n",
    "\n",
    "metrics1 = metrics.accuracy_score(realData[diff:],pred.round(),normalize=True)\n",
    "metrics2 = metrics.accuracy_score(realData[diff:],pred.round(),normalize=False)\n",
    "\n",
    "print(metrics1)\n",
    "print(metrics2)\n",
    "\n",
    "\n",
    "\\end{lstlisting}\n",
    "\n",
    "\\end{appendices}\n",
    "\n",
    "%TC:endignore\n",
    "\\end{document}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
